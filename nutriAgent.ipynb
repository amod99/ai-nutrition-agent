{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d05d156-fb0c-4850-8883-f5586a9b5b4f",
   "metadata": {},
   "source": [
    "# Local Fine-Tuning: Nutritionist Agent for Glucose Spike Analysis\n",
    "\n",
    "This project focuses on fine-tuning a **Llama 3.2 3B** model to act as a specialized Nutrition Analyst. The model is trained to predict metabolic impacts directly from natural language meal descriptions, eliminating the need for external database lookups during inference.\n",
    "\n",
    "## Key Objectives\n",
    "* **Synthetic Data Generation:** Create high-quality instruction pairs for metabolic reasoning.\n",
    "* **Efficient Fine-Tuning:** Utilize **QLoRA** and **Unsloth** for rapid local training.\n",
    "* **Agentic Deployment:** Host the expert model locally via **Ollama** for seamless integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc875ae-c1c3-4c8c-b3c1-2fd2774c3a5b",
   "metadata": {},
   "source": [
    "### 1. Synthetic Data Generation with Pydantic\n",
    "To ensure high-quality training data, use Pydantic to enforce a structured schema for your synthetic nutritionist's thought process. You can use a larger model like Gemini 2.5 Flash as a \"teacher\" to generate thousands of examples of meal logs paired with analytical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ea446-675d-446d-a40c-13d80ef7b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class SyntheticNutritionRecord(BaseModel):\n",
    "    meal_description: str = Field(description=\"Natural language description of the meal\")\n",
    "    glucose_impact: float = Field(description=\"Simulated glucose spike in mg/dL\")\n",
    "    analytical_reasoning: str = Field(description=\"Step-by-step logic linking food components to the spike\")\n",
    "\n",
    "# We wrap individual records in a collection for batch generation\n",
    "class SyntheticBatch(BaseModel):\n",
    "    records: List[SyntheticNutritionRecord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360792a1-8f4d-41d4-84a1-3f52ce1e931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Setup Gemini for structured output\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "full_dataset = []\n",
    "\n",
    "for i in range(50):  # Generate 50 batches of 10\n",
    "    response = model.generate_content(\n",
    "        \"Generate 10 diverse synthetic nutrition logs. Themes: [Randomize Themes]\",\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=SyntheticBatch\n",
    "        )\n",
    "    )\n",
    "    batch = SyntheticBatch.model_validate_json(response.text)\n",
    "    full_dataset.extend(batch.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a05b15-edd1-48ac-bdf1-1141d459ec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SyntheticNutritionRecord(meal_description='Breakfast: Instant oatmeal with a banana.', glucose_impact=65.0, analytical_reasoning='The instant oatmeal contains processed carbohydrates that are quickly digested and absorbed, leading to a rapid rise in blood glucose. The banana adds simple sugars (fructose, glucose) which further contribute to the immediate spike. Though a small amount of fat and protein might be present, their buffering effect is minimal compared to the high glycemic load of the oatmeal and fruit.'),\n",
       " SyntheticNutritionRecord(meal_description='Lunch: Whole wheat pasta with chicken breast and tomato sauce.', glucose_impact=90.0, analytical_reasoning='This meal is high in complex carbohydrates from the whole wheat pasta, which will cause a significant glucose rise, albeit slower than simple sugars. The tomato sauce contains natural sugars, adding to the carbohydrate load. Chicken breast provides protein, and olive oil provides fat, both of which can slow glucose absorption slightly, but the overall carbohydrate quantity is the primary driver of the spike.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6308fc-6351-40af-a949-e094b151e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_to_gemini_jsonl(synthetic_records, output_file=\"gemini_finetune_data.jsonl\"):\n",
    "    \"\"\"\n",
    "    Converts a list of synthetic records into Gemini-compatible JSONL format.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for record in synthetic_records:\n",
    "            # Construct the Gemini-specific nested structure\n",
    "            gemini_entry = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"parts\": [{\"text\": record.meal_description}] # From your Pydantic model\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"model\", \n",
    "                        \"parts\": [{\"text\": f\"Reasoning: {record.analytical_reasoning}\\nImpact: {record.glucose_impact} mg/dL\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            # Write as a single line\n",
    "            f.write(json.dumps(gemini_entry) + '\\n')\n",
    "    \n",
    "    print(f\"✅ Created {len(synthetic_records)} training examples in {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78579e03-3f4e-4d0c-af32-15bbd9abb4c5",
   "metadata": {},
   "source": [
    "### 3. Data Refinement & Filtering\n",
    "Before fine-tuning,  filter synthetic data to ensure quality:\n",
    "\n",
    "Heuristic Filtering: Remove exact duplicates or records with missing fields.\n",
    "\n",
    "Self-Correction: Using second LLM pass (or the same teacher model) to \"criticize\" and correct the reasoning of the generated records.\n",
    "\n",
    "Format for Fine-Tuning: Saved validated records in a JSONL format, where each line is a single JSON object containing an instruction-response pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaba66b7-b751-4766-b362-3ad5fcd58d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 500 training examples in gemini_finetune_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "export_to_gemini_jsonl(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc4d0b9b-3408-4424-bed5-eb74a6f94cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_jsonl(full_dataset, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits a list of synthetic records into training and validation JSONL files.\n",
    "    \"\"\"\n",
    "    # 1. Perform the split (90% Train, 10% Validation by default)\n",
    "    # Using random_state ensures reproducibility for your research repo\n",
    "    train_data, val_data = train_test_split(\n",
    "        full_dataset, \n",
    "        test_size=val_size, \n",
    "        random_state=42, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb65a16-0c89-4404-9d85-e209c09c6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = split_jsonl(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2688a0ca-7692-43a5-9f14-61ead29e542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 450 training examples in train.jsonl\n",
      "✅ Created 50 training examples in val.jsonl\n"
     ]
    }
   ],
   "source": [
    "export_to_gemini_jsonl(train_data, output_file=\"train.jsonl\")\n",
    "export_to_gemini_jsonl(val_data, output_file=\"val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40617dfa-0507-4e79-b1e2-d47177623b0b",
   "metadata": {},
   "source": [
    "## Fine-Tuning Methodology\n",
    "\n",
    "To achieve high-performance results on a consumer-grade GPU, we employ the following optimization stack:\n",
    "\n",
    "### 1. QLoRA (4-bit Quantization)\n",
    "We use **4-bit quantization** combined with **Low-Rank Adaptation (LoRA)**. \n",
    "* **Benefit:** Reduces VRAM requirements significantly.\n",
    "* **Impact:** A 3B parameter model can be trained with as little as **12GB–16GB of RAM**.\n",
    "\n",
    "### 2. Unsloth Optimization\n",
    "**Unsloth** is the industry standard for fast local fine-tuning.\n",
    "* **Speed:** Training is typically **2x–3x faster** than standard methods.\n",
    "* **Efficiency:** Reduces VRAM usage by up to **70%**.\n",
    "\n",
    "### 3. Dataset Strategy\n",
    "* **Size:** 100–500 high-quality synthetic examples.\n",
    "* **Focus:** Teaches the model specific tool-calling patterns and metabolic reasoning logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ec115a-a602-40c6-9c5a-6efd2b128003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1923/2488701093.py:2: UserWarning: WARNING: Unsloth should be imported before [trl, transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Laptop GPU. Num GPUs = 1. Max memory: 7.96 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Configuration\n",
    "max_seq_length = 2048 \n",
    "dtype = None # Auto-detect (Float16 for older GPUs, Bfloat16 for RTX 30/40 series)\n",
    "load_in_4bit = True # Essential for local consumer GPUs\n",
    "\n",
    "# 2. Load Model & Tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 3. Add LoRA Adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Rank: higher = more parameters but more memory\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Optimized for 0\n",
    "    bias = \"none\",    # Optimized for \"none\"\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces VRAM usage\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8540fe48-6b97-4aa0-8a92-3e0bba96671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 450/450 [00:00<00:00, 13499.92 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 8165.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Data Preparation\n",
    "# Load your local JSONL files\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"test\": \"val.jsonl\"})\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Ensure your tokenizer is using the correct Llama 3.2 template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\", # Use llama-3.1 or llama-3.2 depending on your model\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # 'contents' is the top-level list in your Gemini-style dataset\n",
    "    all_contents = examples[\"contents\"] \n",
    "    texts = []\n",
    "    \n",
    "    for conversation in all_contents:\n",
    "        # 1. Map Gemini roles ('model') to Llama 3.2 roles ('assistant')\n",
    "        formatted_messages = []\n",
    "        for turn in conversation:\n",
    "            role = \"user\" if turn[\"role\"] == \"user\" else \"assistant\"\n",
    "            # 2. Join all parts into a single string\n",
    "            content = \" \".join([part[\"text\"] for part in turn[\"parts\"]])\n",
    "            formatted_messages.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        # 3. Apply the Llama 3.2 chat template\n",
    "        # This adds the necessary <|begin_of_text|> and <|eot_id|> tokens\n",
    "        rendered_text = tokenizer.apply_chat_template(\n",
    "            formatted_messages, \n",
    "            tokenize = False, \n",
    "            add_generation_prompt = False\n",
    "        )\n",
    "        texts.append(rendered_text)\n",
    "        \n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply the map \n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7e64a71-2795-4a16-a7d4-06b7e6c2df6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|███████████████████████████████████████████████████████| 450/450 [00:00<00:00, 519.89 examples/s]\n",
      "Map (num_proc=2): 100%|██████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 64.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Training Setup\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Set to 60 for a quick test; use num_train_epochs=1 for full pass\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Saves VRAM\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd88e1a9-1339-4049-8bc7-8f3ad66f76bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 450 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:13, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.928600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.959100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.919300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6. Execute Training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 7. Local Saving\n",
    "model.save_pretrained(\"lora_model\") \n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64ea51-eabf-4822-8af2-5d006395227c",
   "metadata": {},
   "source": [
    "### Testing inference locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1fd900a-9025-441c-9bbe-d7d1e1856eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Laptop GPU. Num GPUs = 1. Max memory: 7.96 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model\", # Load your locally saved model\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable 2x faster inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f531ffe-6cff-4e4b-8265-3c90d9c1c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Analyze my meal: 2 slices of white bread and grape juice.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Reasoning: White bread is a refined carbohydrate that is quickly broken down into glucose, leading to a rapid and significant increase. Grape juice, although natural, is also a sugar-rich beverage with minimal fiber content. This combination results in a very high and immediate glucose response.\n",
      "Impact: 120.0 mg/dL<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Analyze my meal: 2 slices of white bread and grape juice.\"}],\n",
    "    tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe14b29-4e8f-4ea5-9ce0-6eece654bff8",
   "metadata": {},
   "source": [
    "### Export Fine-Tuned Model to GGUF\n",
    "Fine-tuning with Unsloth creates \"LoRA adapters.\" To use them with a local server, you must merge these adapters with the base model and export them to the GGUF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "366c31b1-6521-4bb5-a529-79ef8a51ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /home/amod/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|███████████████████████████████████████| 2/2 [00:00<00:00, 27962.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/amod/synth_health/my_nutrition_model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: llama.cpp folder exists but binaries not found - will rebuild\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['Llama-3.2-3B-Instruct.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['Llama-3.2-3B-Instruct.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model Llama-3.2-3B-Instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'my_nutrition_model',\n",
       " 'gguf_files': ['Llama-3.2-3B-Instruct.Q4_K_M.gguf'],\n",
       " 'modelfile_location': '/home/amod/synth_health/Modelfile',\n",
       " 'want_full_precision': False,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge and export to GGUF format for local hosting\n",
    "model.save_pretrained_gguf(\"my_nutrition_model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9bf02-64ba-4050-a9e6-805d0640e7b7",
   "metadata": {},
   "source": [
    "#### Step 2: Create an Ollama Modelfile\n",
    "Ollama uses a \"Modelfile\" to configure how a model should behave, including its system prompt and template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6425adf-f688-4509-a316-c0900a2acd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Modelfile\n",
    "FROM ./Llama-3.2-3B-Instruct.Q4_K_M.gguf\n",
    "SYSTEM \"You are a professional Nutritionist Agent. When you receive food logs, use your fine-tuned logic to analyze glucose impact.\"\n",
    "PARAMETER temperature 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad33a2-c56a-46f4-905e-6ac7da1816ad",
   "metadata": {},
   "source": [
    "### Phase 3: Deployment\n",
    "The saved model is exported to GGUF format and hosted via **Ollama** to serve as the core \"brain\" for the nutrition agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7588d-3311-4a1b-8b34-d800ce947652",
   "metadata": {},
   "source": [
    "### 1. Create the model in Ollama\n",
    "`ollama create nutrition-agent -f Modelfile`\n",
    "\n",
    "### 2. Start the local server\n",
    "`ollama serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96ac39-70bf-4757-bb9b-b11d51500dc9",
   "metadata": {},
   "source": [
    "### Setup an agent to call our fine tuned model to analyze meal descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fec1f7e1-c8b2-4117-80aa-ab2bac77cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def nutrition_agent(user_meal):\n",
    "    # The agent calls your specialized model directly\n",
    "    # No external database lookup is needed because the model has 'learned' the spikes.\n",
    "    response = ollama.chat(\n",
    "        model='nutrition-agent',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a Nutrition Analyst. Predict glucose spikes based on your fine-tuned knowledge.'},\n",
    "            {'role': 'user', 'content': f\"Analyze my meal: {user_meal}\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the direct prediction from the model's weights\n",
    "    return response['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd59efec-6c9d-4bfb-9f9e-a2766d7935d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The high refined carbohydrate load (oats, white rice) and lactose sugar from milk. This is an extremely pure simple sugar absorption without any direct fat or fiber to buffer.\n",
      "Impact: 95.0.0 mg/dL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meal_description = \"granola with milk\"\n",
    "print(nutrition_agent(meal_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ab750-ac4c-4538-9f02-c84044694ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae2ca1-e0ee-40bb-be7b-6f950b6e1fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph",
   "language": "python",
   "name": "langraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
